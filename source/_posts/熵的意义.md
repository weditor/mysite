---
title: 信息熵
mathjax: true
date: 2019-08-27 23:08:39
---

# 信息熵的公式 

$$
H(x) = -\sum_0^n(p_i*\log p_i)
$$

<!-- more -->

信息熵通常代表一段序列包含的信息量的大小。如果序列容易预测，表示所含的信息量很低，相反不容易预测，就称为信息量大。

进一步讲，概率越小的事情发生了，越超越人类认知，包含的信息量也越大，比如月球撞击地球。

# 理解信息熵


信息熵的关键在于理解 $log p_i$ .

假设一组事件中的单个事件 i 发生的概率是 $p_i$ 。信息熵的含义是当 i 发生时，一个人猜中i发生所需要的平均猜测次数。

比如一个骰子某次是1朝上，每次猜测对方只会回答是/否。最有效率的方法，应该按照以下二分法猜测：
1. 骰子点数大于等于4？ => 否
2. 点数大于等于2? => 否
3. 点数是 1 => 是

显然, 单个事件发生后，所需要的猜测次数:
$$
C_i = \log_2 (1/p_i) = -\log_2 p_i
$$

而每个事件 i 发生的概率为 $p_i$

所以，平均猜测次数的期望为 
$$
\begin{aligned}
E(C) & = \sum_0^n(p_i * C_i) \\
    & = \sum_0^n(p_i * (-\log_2 p_i)) \\
    & = -\sum_0^n(p_i * \log_2 p_i)
\end{aligned}
$$

其中 log 的底可以取其他值，取不同的值会把最后的熵等比例放大，物理意义就是每猜测一次允许分开的区间, 
即上面的猜测方法更改为二分法/三分法/四分法……
